<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../style.css">
    <base target="_parent">
  </head>
  
  <body>
    <div class="content">
       <h1>Accuracy Methods</h1>

      <div class="ck-content">
        <p style="margin-left:0px;">Accuracy methods are metrics to evaluate a classification model's performance,
          focusing on correctly classified instances. Common methods include:</p>
        <ol>
          <li><strong>Accuracy Score</strong>: The ratio of correctly classified instances
            to total instances, best for balanced datasets.</li>
          <li><strong>Confusion Matrix</strong>: Shows true positive, true negative,
            false positive, and false negative counts, leading to precision, recall,
            and F1-score metrics.</li>
          <li><strong>Precision and Recall</strong>: Precision is the correctly predicted
            positive instances ratio among all positive predictions. Recall measures
            correctly predicted positives out of actual positives.</li>
          <li><strong>F1-Score</strong>: Harmonic mean of precision and recall, balancing
            both and useful for imbalanced datasets.</li>
          <li><strong>ROC Curve</strong>: Graphical representation showing the trade-off
            between true positive rate and false positive rate, with Area Under the
            Curve (AUC-ROC) as a performance indicator.</li>
          <li><strong>Cross-Validation</strong>: Evaluates model performance on unseen
            data by training and testing on different data subsets, helping to prevent
            overfitting.</li>
        </ol>
      </div>
    </div>
  </body>

</html>