<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../../../style.css">
    <base target="_parent">
  </head>
  
  <body>
    <div class="content">
       <h1>SGD (Stochastic Gradient Descent)</h1>

      <div class="ck-content">
        <p style="margin-left:0px;">Stochastic Gradient Descent (SGD) is a variant of the gradient descent
          optimization algorithm in machine learning. Unlike traditional gradient
          descent, which uses the entire training dataset to update parameters, SGD
          uses gradients from a single or a small batch of randomly selected instances
          for more frequent updates. This makes it more efficient, especially with
          large datasets, often leading to faster convergence.</p>
        <p style="margin-left:0px;">Each iteration of SGD involves randomly selecting training data, computing
          gradients, and updating parameters in the negative gradient direction,
          scaled by a learning rate. The learning rate is critical for balancing
          convergence speed and stability, as a too large rate can cause overshooting
          the minimum of the cost function.</p>
      </div>
    </div>
  </body>

</html>