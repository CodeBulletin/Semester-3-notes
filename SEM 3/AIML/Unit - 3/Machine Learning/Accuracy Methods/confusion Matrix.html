<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../../style.css">
    <base target="_parent">
  </head>
  
  <body>
    <div class="content">
       <h1>confusion Matrix</h1>

      <div class="ck-content">
        <p>A confusion matrix is a table that is used to evaluate the performance
          of a classification model. It provides a detailed breakdown of the model's
          predictions and the actual class labels of the data.</p>
        <p>A confusion matrix is typically represented as a square matrix, where
          the rows correspond to the actual class labels and the columns correspond
          to the predicted class labels. The diagonal elements of the matrix represent
          the correctly classified instances, while the off-diagonal elements represent
          the misclassified instances.</p>
        <figure class="image">
          <img src="confusion%20Matrix/image.jpg">
        </figure>
        <p>The confusion matrix allows us to calculate various evaluation metrics,
          such as accuracy, precision, recall, and F1-score.</p>
        <ol>
          <li><strong>Accuracy</strong>: It measures the proportion of correctly classified
            instances out of the total number of instances. It is calculated as (True
            Positive + True Negative) / Total.</li>
          <li><strong>Precision</strong>: It measures the proportion of correctly predicted
            positive instances out of all instances predicted as positive. It is calculated
            as True Positive / (True Positive + False Positive).</li>
          <li><strong>Recall (Sensitivity or True Positive Rate)</strong>: It measures
            the proportion of correctly predicted positive instances out of all actual
            positive instances. It is calculated as True Positive / (True Positive
            + False Negative).</li>
          <li><strong>F1-score</strong>: It is the harmonic mean of precision and recall.
            It provides a balanced measure of a model's performance by considering
            both precision and recall. It is calculated as 2 * (Precision * Recall)
            / (Precision + Recall).</li>
        </ol>
        <p>The confusion matrix provides a more detailed analysis of the model's
          performance beyond simple accuracy. It helps identify the types of errors
          made by the model, such as false positives and false negatives, and allows
          for a deeper understanding of the model's strengths and weaknesses.</p>
      </div>
    </div>
  </body>

</html>