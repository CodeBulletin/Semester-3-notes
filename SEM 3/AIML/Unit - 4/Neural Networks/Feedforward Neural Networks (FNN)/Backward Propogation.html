<html>
  
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../../../style.css">
    <base target="_parent">
  </head>
  
  <body>
    <div class="content">
       <h1>Backward Propogation</h1>

      <div class="ck-content">
        <p style="margin-left:0px;">Backpropagation is a key algorithm in neural network training, used for
          optimizing weights by computing the gradient of the loss function. The
          process includes:</p>
        <ol>
          <li><strong>Forward Pass:</strong> Calculate each layer's output using current
            weights and biases.</li>
          <li><strong>Loss Function:</strong> After the forward pass, compare the network's
            output to the desired output and compute the loss using functions like
            Mean Squared Error (MSE) or Cross-Entropy Loss.</li>
          <li><strong>Backward Pass:</strong> Move backward from the output to the input
            layer, computing the loss gradient for each weight.</li>
          <li><strong>Compute Gradients:</strong>
            <ul>
              <li><strong>Output Layer:</strong> Determine the loss function's derivative
                with respect to the network's output.</li>
              <li><strong>Hidden Layers:</strong> Use the chain rule to calculate each layer's
                loss gradient, propagating from the output layer backward.</li>
            </ul>
          </li>
          <li><strong>Chain Rule Application:</strong> Calculate the loss function gradients
            relative to weights and biases, involving:
            <ul>
              <li>The loss derivative with respect to the neuron's output connected to the
                weight.</li>
              <li>The activation of the preceding neuron.</li>
              <li>The derivative of the activation function in the receiving neuron.</li>
            </ul>
          </li>
          <li><strong>Update Weights:</strong> Use gradients to update weights and biases,
            typically with gradient descent, adjusting weights in the opposite direction
            of the gradient.</li>
          <li><strong>Iterations:</strong> Repeat the process over multiple iterations
            (epochs) to refine weights, enhancing the network's output accuracy.</li>
        </ol>
        <p style="margin-left:0px;">Backpropagation allows the network to learn from errors and improve performance
          iteratively.</p>
      </div>
    </div>
  </body>

</html>